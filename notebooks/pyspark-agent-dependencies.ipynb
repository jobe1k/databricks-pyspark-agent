{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a29e6d7f-753c-4bcb-ad60-43df2ea3dc62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd9a3da6-6bef-48bf-ad9b-178a0b2ec047",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os, re, json, ast, traceback\n",
    "from typing import List, Dict, Any, Optional, TypedDict, Tuple\n",
    "from dataclasses import dataclass\n",
    "from jsonschema import validate as jsonschema_validate, Draft202012Validator\n",
    "from pydantic import BaseModel, Field\n",
    "from functools import reduce\n",
    "\n",
    "from pyspark.sql import DataFrame, functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "\n",
    "# LangChain / LangGraph\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import AnyMessage\n",
    "from langchain_community.chat_models import ChatDatabricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8919ded9-d87c-463e-99c8-48c51f114426",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Utilities and helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "72eb5780-0636-4149-b30f-158776136a2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def strip_code_fences(code: str) -> str:\n",
    "    \"\"\"Return the inner code by removing triple-backtick fences.\n",
    "\n",
    "    Supports both ```...``` and ```python ...``` styles and leaves input unchanged\n",
    "    if no fences exist. This is intentionally minimal—just enough to clean LLM\n",
    "    output before further processing.\n",
    "\n",
    "    Args:\n",
    "        code: Raw text that may contain fenced code.\n",
    "\n",
    "    Returns:\n",
    "        The unfenced code string. If `code` is falsy, returns it as-is.\n",
    "\n",
    "    Examples:\n",
    "        >>> strip_code_fences(\"```python\\\\nprint(1)\\\\n```\")\n",
    "        'print(1)'\n",
    "    \"\"\"\n",
    "    if not code:\n",
    "        return code\n",
    "    m = re.search(r\"```(?:python)?\\s*(.*?)\\s*```\", code, flags=re.DOTALL | re.IGNORECASE)\n",
    "    return m.group(1) if m else code\n",
    "\n",
    "\n",
    "def sanitize_pyspark_code(code: str) -> str:\n",
    "    \"\"\"Normalize/clean a PySpark snippet for safe parsing and execution.\n",
    "\n",
    "    What this does:\n",
    "    - Strips code fences and Markdown artifacts (bold, backticks).\n",
    "    - Removes *all* import lines (we provide `df` and `F`).\n",
    "    - Trims leading/trailing blank lines.\n",
    "    - Normalizes indentation by removing the minimum common indent.\n",
    "\n",
    "    This function is intentionally conservative; it does not rewrite logic or\n",
    "    attempt to \"fix\" broken code—just gets it into a predictable, safe shape\n",
    "    for AST checks and `exec`.\n",
    "\n",
    "    Args:\n",
    "        code: Arbitrary code block (often from an LLM).\n",
    "\n",
    "    Returns:\n",
    "        A cleaned, trimmed snippet suitable for static checks and execution.\n",
    "\n",
    "    Examples:\n",
    "        >>> sanitize_pyspark_code(\"```\\\\nfrom x import y\\\\n df_out = df\\\\n```\")\n",
    "        'df_out = df'\n",
    "    \"\"\"\n",
    "    code = strip_code_fences(code or \"\")\n",
    "    safe_lines: List[str] = []\n",
    "\n",
    "    for line in code.splitlines():\n",
    "        # Drop any import-style statements (including `from ... import ...`)\n",
    "        if re.match(r\"^\\s*(import|from)\\s+\", line):\n",
    "            continue\n",
    "\n",
    "        # Remove Markdown bold **text** and inline code `text`\n",
    "        line = re.sub(r'\\*\\*(.*?)\\*\\*', r'\\1', line)\n",
    "        line = re.sub(r'`([^`]+)`', r'\\1', line)\n",
    "\n",
    "        # Skip initial empty lines for a tidy block\n",
    "        if not safe_lines and not line.strip():\n",
    "            continue\n",
    "\n",
    "        safe_lines.append(line)\n",
    "\n",
    "    if not safe_lines:\n",
    "        return \"\"\n",
    "\n",
    "    # Remove trailing empties\n",
    "    while safe_lines and not safe_lines[-1].strip():\n",
    "        safe_lines.pop()\n",
    "\n",
    "    # Normalize indentation by removing minimum common leading whitespace\n",
    "    code_text = \"\\n\".join(safe_lines)\n",
    "    lines_with_content = [l for l in safe_lines if l.strip()]\n",
    "    if lines_with_content:\n",
    "        min_indent = min(len(l) - len(l.lstrip()) for l in lines_with_content)\n",
    "        normalized_lines: List[str] = []\n",
    "        for l in safe_lines:\n",
    "            normalized_lines.append(l[min_indent:] if l.strip() and len(l) > min_indent else (l if l.strip() else \"\"))\n",
    "        code_text = \"\\n\".join(normalized_lines)\n",
    "\n",
    "    return code_text.strip()\n",
    "\n",
    "\n",
    "def _normalize_dbfs_path(p: str) -> str:\n",
    "    \"\"\"Normalize DBFS-style paths so Spark readers/writers accept them.\n",
    "\n",
    "    Converts `/dbfs/...` to `dbfs:/...`. S3/ABFSS/etc. URIs pass through unchanged.\n",
    "\n",
    "    Args:\n",
    "        p: A local/DBFS/cloud path string.\n",
    "\n",
    "    Returns:\n",
    "        A normalized path string suitable for Spark IO.\n",
    "    \"\"\"\n",
    "    if not p:\n",
    "        return p\n",
    "    p = p.strip()\n",
    "    if p.startswith(\"/dbfs/\"):\n",
    "        return \"dbfs:\" + p[5:]\n",
    "    return p\n",
    "\n",
    "\n",
    "def _infer_format(path: str, user_fmt: str) -> str:\n",
    "    \"\"\"Infer input format from file suffix, honoring a user override.\n",
    "\n",
    "    Args:\n",
    "        path: Path that may carry a meaningful extension.\n",
    "        user_fmt: Optional explicit format (e.g., \"parquet\", \"delta\").\n",
    "                  If not \"auto\", it takes precedence.\n",
    "\n",
    "    Returns:\n",
    "        One of {\"parquet\",\"delta\",\"json\",\"csv\"} with \"parquet\" as a safe default.\n",
    "    \"\"\"\n",
    "    if user_fmt and user_fmt.lower() != \"auto\":\n",
    "        return user_fmt.lower()\n",
    "    p = (path or \"\").lower()\n",
    "    if p.endswith(\".parquet\"):\n",
    "        return \"parquet\"\n",
    "    if p.endswith(\".delta\"):\n",
    "        return \"delta\"\n",
    "    if p.endswith(\".json\"):\n",
    "        return \"json\"\n",
    "    if p.endswith(\".csv\"):\n",
    "        return \"csv\"\n",
    "    return \"parquet\"  # default columnar choice\n",
    "\n",
    "\n",
    "def read_df_from_path(path: str, fmt: str) -> DataFrame:\n",
    "    \"\"\"Read a dataset into a Spark DataFrame with light format handling.\n",
    "\n",
    "    Notes:\n",
    "        - Requires a live `spark` session in the current runtime.\n",
    "        - CSV reads with header inference; other formats use Spark defaults.\n",
    "\n",
    "    Args:\n",
    "        path: Input path (DBFS/S3/ABFSS/etc.).\n",
    "        fmt: Requested format or \"auto\" (will be inferred via `_infer_format`).\n",
    "\n",
    "    Returns:\n",
    "        A Spark DataFrame loaded from the provided path.\n",
    "    \"\"\"\n",
    "    path = _normalize_dbfs_path(path)\n",
    "    fmt = _infer_format(path, fmt)\n",
    "    reader = spark.read  # type: ignore[name-defined]\n",
    "\n",
    "    if fmt == \"csv\":\n",
    "        return reader.option(\"header\", True).option(\"inferSchema\", True).csv(path)\n",
    "    if fmt == \"json\":\n",
    "        return reader.json(path)\n",
    "    if fmt == \"delta\":\n",
    "        return reader.format(\"delta\").load(path)\n",
    "    if fmt == \"parquet\":\n",
    "        return reader.parquet(path)\n",
    "\n",
    "    # Last resort: try parquet\n",
    "    return reader.parquet(path)\n",
    "\n",
    "\n",
    "def load_inputs(paths_csv: str, fmt: str) -> DataFrame:\n",
    "    \"\"\"Load one or more paths and union-by-name across a superset schema.\n",
    "\n",
    "    Pragmatic behavior for real-world data drift:\n",
    "    - Reads each path, collects the union of all columns.\n",
    "    - Adds missing columns (nullable string) so unionByName succeeds.\n",
    "    - Returns a single DataFrame of all rows.\n",
    "\n",
    "    Args:\n",
    "        paths_csv: Comma-separated list of input paths.\n",
    "        fmt: Input format or \"auto\" for suffix-based inference.\n",
    "\n",
    "    Returns:\n",
    "        A Spark DataFrame combining all inputs.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If no usable paths are provided.\n",
    "    \"\"\"\n",
    "    paths = [p.strip() for p in (paths_csv or \"\").split(\",\") if p.strip()]\n",
    "    if not paths:\n",
    "        raise ValueError(\"No input paths provided.\")\n",
    "\n",
    "    dfs = [read_df_from_path(p, fmt) for p in paths]\n",
    "    if len(dfs) == 1:\n",
    "        return dfs[0]\n",
    "\n",
    "    # Build superset schema across ALL inputs, then unionByName with allowMissingColumns\n",
    "    all_cols = sorted(list(set().union(*[set(df.columns) for df in dfs])))\n",
    "    aligned: List[DataFrame] = []\n",
    "    for df in dfs:\n",
    "        missing = [c for c in all_cols if c not in df.columns]\n",
    "        for c in missing:\n",
    "            df = df.withColumn(c, F.lit(None).cast(StringType()))\n",
    "        aligned.append(df.select(all_cols))\n",
    "\n",
    "    return reduce(lambda a, b: a.unionByName(b, allowMissingColumns=True), aligned)\n",
    "\n",
    "\n",
    "def schema_preview(df: DataFrame, max_cols: int = 40) -> str:\n",
    "    \"\"\"Return a human-friendly preview of `df.dtypes`, clipped for prompt context.\n",
    "\n",
    "    Args:\n",
    "        df: Input Spark DataFrame.\n",
    "        max_cols: Maximum number of columns to include before clipping.\n",
    "\n",
    "    Returns:\n",
    "        A multi-line string of `col: dtype` lines (with an ellipsis sentinel if clipped).\n",
    "    \"\"\"\n",
    "    dtypes = df.dtypes\n",
    "    if len(dtypes) > max_cols:\n",
    "        dtypes = dtypes[:max_cols] + [(\"…\", \"…\")]\n",
    "    return \"\\n\".join([f\"{c}: {t}\" for c, t in dtypes])\n",
    "\n",
    "\n",
    "def exec_pyspark_code(pyspark_code: str, df: DataFrame) -> DataFrame:\n",
    "    \"\"\"Execute a sanitized PySpark snippet with a constrained namespace.\n",
    "\n",
    "    Contract:\n",
    "      - Provides `df` (input DataFrame) and `F` (pyspark.sql.functions).\n",
    "      - Snippet must assign the final result to `df_out`.\n",
    "      - Raises if `df_out` is missing.\n",
    "\n",
    "    Security posture:\n",
    "      Assumes code was pre-validated (e.g., via AST checks) to forbid IO and\n",
    "      dangerous calls. This function focuses on controlled execution, not policing.\n",
    "\n",
    "    Args:\n",
    "        pyspark_code: A cleaned, ready-to-exec PySpark snippet.\n",
    "        df: The input DataFrame to operate on.\n",
    "\n",
    "    Returns:\n",
    "        The `df_out` DataFrame produced by the snippet.\n",
    "\n",
    "    Raises:\n",
    "        RuntimeError: If the snippet fails to set `df_out`.\n",
    "    \"\"\"\n",
    "    local_env: Dict[str, Any] = {\"df\": df, \"F\": F}\n",
    "    global_env: Dict[str, Any] = {}\n",
    "    exec(pyspark_code, global_env, local_env)  # sandboxed dicts\n",
    "    if \"df_out\" not in local_env:\n",
    "        raise RuntimeError(\"Code did not produce `df_out`.\")\n",
    "    return local_env[\"df_out\"]  # type: ignore[no-any-return]\n",
    "\n",
    "\n",
    "def parse_llm_json(s: str) -> Dict[str, Any]:\n",
    "    \"\"\"Extract a JSON object from messy LLM text (fences and leading/trailing noise).\n",
    "\n",
    "    This is a pragmatic parser: look for ```json fenced blocks first; if not present,\n",
    "    find the first/last brace pair and parse that slice.\n",
    "\n",
    "    Args:\n",
    "        s: Raw model output possibly containing JSON.\n",
    "\n",
    "    Returns:\n",
    "        The parsed JSON object.\n",
    "\n",
    "    Raises:\n",
    "        json.JSONDecodeError: If the extracted slice cannot be parsed as JSON.\n",
    "    \"\"\"\n",
    "    s = (s or \"\").strip()\n",
    "    m = re.search(r\"```(?:json)?\\s*(.*?)\\s*```\", s, flags=re.DOTALL | re.IGNORECASE)\n",
    "    if m:\n",
    "        s = m.group(1).strip()\n",
    "    first, last = s.find(\"{\"), s.rfind(\"}\")\n",
    "    if first != -1 and last != -1 and last > first:\n",
    "        s = s[first:last+1]\n",
    "    return json.loads(s)\n",
    "\n",
    "\n",
    "def clean_dict_keys(d: Any) -> Any:\n",
    "    \"\"\"Recursively strip stray quotes from dict keys and string values.\n",
    "\n",
    "    Some models produce objects like {'\"verdict\"': '\"approve\"'}—which is syntactically\n",
    "    valid but annoying. This normalizes keys/values without altering structure.\n",
    "\n",
    "    Args:\n",
    "        d: Any nested structure (dict/list/scalars).\n",
    "\n",
    "    Returns:\n",
    "        The same structure with normalized keys and string values.\n",
    "    \"\"\"\n",
    "    if isinstance(d, dict):\n",
    "        cleaned: Dict[Any, Any] = {}\n",
    "        for k, v in d.items():\n",
    "            clean_k = k.strip().strip('\"').strip(\"'\") if isinstance(k, str) else k\n",
    "            cleaned[clean_k] = clean_dict_keys(v)\n",
    "        return cleaned\n",
    "    if isinstance(d, list):\n",
    "        return [clean_dict_keys(item) for item in d]\n",
    "    if isinstance(d, str):\n",
    "        return d.strip().strip('\"').strip(\"'\") if d.startswith(('\"', \"'\")) else d\n",
    "    return d\n",
    "\n",
    "\n",
    "def normalize_validator_report(raw: Any) -> Dict[str, Any]:\n",
    "    \"\"\"Normalize a validator LLM response into a consistent, easy-to-route dict.\n",
    "\n",
    "    Target shape:\n",
    "        {\n",
    "          \"verdict\": \"approve\" | \"revise\",\n",
    "          \"reasons\": str,\n",
    "          \"suggested_edits\": Optional[str],\n",
    "          ... # passthrough keys preserved\n",
    "        }\n",
    "\n",
    "    Behavior:\n",
    "    - Accepts dicts or strings (tries to parse embedded JSON inside strings).\n",
    "    - Unwraps common nesting keys: result/output/data/response.\n",
    "    - Normalizes key casing and strips stray quotes.\n",
    "    - Falls back to a conservative \"revise\" verdict if parsing fails.\n",
    "\n",
    "    Args:\n",
    "        raw: Arbitrary LLM output (string or dict-like).\n",
    "\n",
    "    Returns:\n",
    "        A normalized dictionary suitable for downstream routing and logging.\n",
    "    \"\"\"\n",
    "    default: Dict[str, Any] = {\n",
    "        \"verdict\": \"revise\",\n",
    "        \"reasons\": \"Failed to parse validator response\",\n",
    "        \"suggested_edits\": None,\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        print(f\"DEBUG normalize_validator_report: Input type: {type(raw)}\")\n",
    "        print(f\"DEBUG normalize_validator_report: Input preview: {str(raw)[:200]}\")\n",
    "\n",
    "        # Parse strings; try fenced JSON first, then brace extraction\n",
    "        if isinstance(raw, str):\n",
    "            raw = raw.strip()\n",
    "            if raw.startswith(\"```\"):\n",
    "                raw = re.sub(r\"```(?:json)?\\s*\", \"\", raw)\n",
    "                raw = re.sub(r\"\\s*```\\s*$\", \"\", raw)\n",
    "            try:\n",
    "                raw = json.loads(raw)\n",
    "                print(\"DEBUG: Successfully parsed JSON string\")\n",
    "            except json.JSONDecodeError:\n",
    "                match = re.search(r'\\{[^{}]*(?:\\{[^{}]*\\}[^{}]*)*\\}', raw)\n",
    "                if match:\n",
    "                    try:\n",
    "                        raw = json.loads(match.group(0))\n",
    "                        print(\"DEBUG: Extracted JSON object from text\")\n",
    "                    except Exception:\n",
    "                        return default\n",
    "                else:\n",
    "                    return default\n",
    "\n",
    "        if not isinstance(raw, dict):\n",
    "            return default\n",
    "\n",
    "        # Unwrap common nesting keys\n",
    "        for key in (\"result\", \"output\", \"data\", \"response\"):\n",
    "            for k in (key, f'\"{key}\"', f\"'{key}'\"):\n",
    "                if k in raw and isinstance(raw[k], dict):\n",
    "                    raw = raw[k]\n",
    "                    break\n",
    "\n",
    "        # Normalize keys (lowercase, strip quotes/whitespace)\n",
    "        normalized: Dict[str, Any] = {}\n",
    "        for k, v in list(raw.items()):\n",
    "            if not isinstance(k, str):\n",
    "                continue\n",
    "            ck = k.strip()\n",
    "            while ck and ck[0] in \"\\\"'\" and ck[-1] in \"\\\"'\":\n",
    "                ck = ck[1:-1]\n",
    "            normalized[ck.lower()] = v\n",
    "\n",
    "        # Extract verdict\n",
    "        verdict = None\n",
    "        for vkey in (\"verdict\", \"status\", \"result\"):\n",
    "            if vkey in normalized:\n",
    "                verdict = normalized[vkey]\n",
    "                break\n",
    "        if isinstance(verdict, str):\n",
    "            verdict = verdict.strip().strip('\"').strip(\"'\").lower()\n",
    "        verdict = verdict if verdict in (\"approve\", \"revise\") else \"revise\"\n",
    "\n",
    "        # Reasons + suggested edits\n",
    "        reasons = normalized.get(\"reasons\") or normalized.get(\"reason\") or \"No reasons provided\"\n",
    "        if not isinstance(reasons, str):\n",
    "            reasons = str(reasons)\n",
    "        suggested = normalized.get(\"suggested_edits\") or normalized.get(\"suggested_edit\")\n",
    "        if suggested is not None and not isinstance(suggested, str):\n",
    "            suggested = str(suggested)\n",
    "\n",
    "        out: Dict[str, Any] = {\n",
    "            \"verdict\": verdict,\n",
    "            \"reasons\": reasons,\n",
    "            \"suggested_edits\": suggested,\n",
    "        }\n",
    "\n",
    "        # Pass through any additional keys for debugging/telemetry\n",
    "        for k, v in normalized.items():\n",
    "            if k not in {\"verdict\", \"status\", \"result\", \"reasons\", \"reason\", \"suggested_edits\", \"suggested_edit\"}:\n",
    "                out[k] = v\n",
    "\n",
    "        return out\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"CRITICAL ERROR in normalize_validator_report: {e}\")\n",
    "        return default\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4f53137c-64c3-4011-8973-20ffb1a3ac78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Model selector (Databricks endpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "884dbe00-8742-4530-8068-9c0d2f16d97a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def make_llm(provider: str, temperature: float) -> ChatDatabricks:\n",
    "    \"\"\"Return a Databricks-hosted chat model client for the requested provider.\n",
    "\n",
    "    This helper abstracts over multiple model-serving endpoints (OpenAI, Anthropic, Llama)\n",
    "    so the rest of the pipeline can stay provider-agnostic.\n",
    "\n",
    "    Design intent:\n",
    "        - Normalize provider labels to lowercase.\n",
    "        - Route to a corresponding Databricks Model Serving endpoint.\n",
    "        - Fail fast on unrecognized providers.\n",
    "\n",
    "    Args:\n",
    "        provider: Name of the LLM provider (\"openai\", \"anthropic\", or \"llama\").\n",
    "        temperature: Sampling temperature to control model creativity.\n",
    "\n",
    "    Returns:\n",
    "        ChatDatabricks: A configured chat model client bound to the right endpoint.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the provider is not recognized.\n",
    "\n",
    "    Example:\n",
    "        >>> llm = make_llm(\"openai\", temperature=0.3)\n",
    "        >>> llm.invoke(\"Hello, world!\")\n",
    "    \"\"\"\n",
    "    provider = (provider or \"\").lower()\n",
    "\n",
    "    # Replace these with your actual Databricks Model Serving endpoint names\n",
    "    if provider == \"openai\":\n",
    "        return ChatDatabricks(endpoint=\"gpt-5-chat\", temperature=temperature)\n",
    "    if provider == \"anthropic\":\n",
    "        return ChatDatabricks(endpoint=\"databricks-claude-sonnet-4\", temperature=temperature)\n",
    "    if provider == \"llama\":\n",
    "        return ChatDatabricks(endpoint=\"databricks-llama-4-maverick\", temperature=temperature)\n",
    "\n",
    "    raise ValueError(f\"Unknown provider: {provider}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c13faae8-c201-4eaf-b9c8-e2a713ff3532",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### LLM-Generated Plan Schema & Safety Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "450c71d4-141a-4209-8415-7441d5f8520a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "PLAN_SCHEMA: Dict[str, Any] = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"summary\": {\"type\": \"string\"},\n",
    "        \"pyspark_code\": {\"type\": \"string\"},\n",
    "        \"expected_effects\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"row_change\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"enum\": [\"increase\", \"decrease\", \"similar\", \"unknown\"],\n",
    "                },\n",
    "                \"new_columns\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n",
    "                \"dropped_columns\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n",
    "                \"column_type_changes\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"additionalProperties\": {\"type\": \"string\"},\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\"row_change\", \"new_columns\", \"dropped_columns\"],\n",
    "            \"additionalProperties\": True,\n",
    "        },\n",
    "    },\n",
    "    \"required\": [\"summary\", \"pyspark_code\", \"expected_effects\"],\n",
    "    \"additionalProperties\": False,\n",
    "}\n",
    "\n",
    "#: Fast denylist to short-circuit obviously unsafe snippets *before* AST analysis.\n",
    "#: Keep this blunt and conservative; we prefer false positives over risky code.\n",
    "DENY_PATTERNS: List[str] = [\n",
    "    r\"\\bdf\\.write\\b\",\n",
    "    r\"\\.saveAsTable\\b\",\n",
    "    r\"\\binsertInto\\b\",\n",
    "    r\"\\bspark\\.sql\\s*\\(\",\n",
    "    r\"\\bdbutils\\.\",\n",
    "    r\"shutil\",\n",
    "    r\"os\\.remove\",\n",
    "    r\"subprocess\",\n",
    "    r\"__import__\",\n",
    "    r\"\\bopen\\s*\\(\",\n",
    "    r\"SparkFiles\",\n",
    "    r\"drop\\s+table\",\n",
    "    r\"delete\\s+from\",\n",
    "]\n",
    "\n",
    "\n",
    "def code_is_safe(pyspark_code: str) -> Tuple[bool, str]:\n",
    "    \"\"\"Static safety screening for proposed PySpark snippets.\n",
    "\n",
    "    This is a two-stage filter:\n",
    "      1) **Regex denylist** for quick wins (fast and opinionated).\n",
    "      2) **AST walk** to block dangerous nodes/calls (imports, globals),\n",
    "         plus attribute calls on sensitive objects (`spark`, `dbutils`),\n",
    "         and any write/save-style attributes anywhere in the tree.\n",
    "\n",
    "    We’re intentionally strict: these snippets execute in the same process,\n",
    "    so we block IO and side-effects. If it’s not clearly safe, it’s a “no”.\n",
    "\n",
    "    Args:\n",
    "        pyspark_code: The sanitized code string to check (no imports/fences).\n",
    "\n",
    "    Returns:\n",
    "        (ok, reason):\n",
    "            ok: True if the snippet passes all checks; False otherwise.\n",
    "            reason: \"ok\" on success or a short explanation on failure.\n",
    "\n",
    "    Examples:\n",
    "        >>> code_is_safe(\"df_out = df.select('x')\")\n",
    "        (True, 'ok')\n",
    "        >>> code_is_safe(\"spark.sql('select 1')\")\n",
    "        (False, 'Disallowed attribute call: spark.sql')\n",
    "    \"\"\"\n",
    "    # 1) Quick regex denies\n",
    "    for pat in DENY_PATTERNS:\n",
    "        if re.search(pat, pyspark_code, flags=re.IGNORECASE):\n",
    "            return False, f\"Denied pattern matched: {pat}\"\n",
    "\n",
    "    # 2) AST inspection (imports, dangerous calls, attribute calls)\n",
    "    try:\n",
    "        tree = ast.parse(pyspark_code, mode=\"exec\")\n",
    "        for node in ast.walk(tree):\n",
    "            # Ban import statements and scope trickery\n",
    "            if isinstance(node, (ast.Import, ast.ImportFrom, ast.Global, ast.Nonlocal)):\n",
    "                return False, f\"Disallowed AST node: {type(node).__name__}\"\n",
    "\n",
    "            # Ban dangerous builtins: eval/exec/compile/open/__import__\n",
    "            if isinstance(node, ast.Call):\n",
    "                if isinstance(node.func, ast.Name) and node.func.id in {\n",
    "                    \"eval\",\n",
    "                    \"exec\",\n",
    "                    \"compile\",\n",
    "                    \"open\",\n",
    "                    \"__import__\",\n",
    "                }:\n",
    "                    return False, f\"Disallowed call: {node.func.id}\"\n",
    "\n",
    "                # Ban attribute calls on sensitive roots (spark.*, dbutils.*)\n",
    "                if isinstance(node.func, ast.Attribute):\n",
    "                    if isinstance(node.func.value, ast.Name) and node.func.value.id in {\n",
    "                        \"spark\",\n",
    "                        \"dbutils\",\n",
    "                    }:\n",
    "                        return False, (\n",
    "                            f\"Disallowed attribute call: \"\n",
    "                            f\"{node.func.value.id}.{node.func.attr}\"\n",
    "                        )\n",
    "\n",
    "            # Ban write/save actions anywhere in the attribute chain\n",
    "            if isinstance(node, ast.Attribute) and node.attr in {\n",
    "                \"write\",\n",
    "                \"save\",\n",
    "                \"saveAsTable\",\n",
    "                \"insertInto\",\n",
    "            }:\n",
    "                return False, f\"Disallowed attribute: .{node.attr}\"\n",
    "\n",
    "    except Exception as e:\n",
    "        # If we can’t confidently analyze the code, refuse it.\n",
    "        return False, f\"AST analysis error: {e}\"\n",
    "\n",
    "    return True, \"ok\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d07a9a9b-7358-4384-9a47-83b8d3e72834",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "858cfcc2-70ee-4ba3-810a-453ee4e4931d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "PLANNER_SYS: str = \"\"\"You are a senior PySpark data engineer. \n",
    "Given a user's natural-language instruction and a DataFrame schema preview, generate SAFE PySpark code that:\n",
    "- uses the input `df` as the starting DataFrame,\n",
    "- performs only in-memory transformations,\n",
    "- NEVER writes to disk and NEVER uses spark.sql or dbutils,\n",
    "- ends with a variable named `df_out` assigned to the resulting DataFrame.\n",
    "\n",
    "Return JSON strictly matching this schema:\n",
    "{schema}\n",
    "\"\"\"\n",
    "\n",
    "PLANNER_HUMAN: str = \"\"\"User instruction:\n",
    "```\n",
    "{instruction}\n",
    "```\n",
    "\n",
    "DataFrame schema (dtypes and sample column names):\n",
    "```\n",
    "{schema_text}\n",
    "```\n",
    "\n",
    "Return ONLY the JSON (no prose).\"\"\"\n",
    "\n",
    "VALIDATOR_SYS: str = \"\"\"You are a meticulous code validator and QA agent.\n",
    "Input: a proposed PySpark snippet and claimed effects.\n",
    "Task:\n",
    "1) Check safety (no IO, no spark.sql, only DataFrame ops).\n",
    "2) Check that code likely compiles given the schema preview.\n",
    "3) Predict whether expected effects are plausible.\n",
    "Return a JSON:\n",
    "{{\"verdict\":\"approve\"|\"revise\",\"reasons\":\"...\", \"suggested_edits\":\"(optional code if revise)\"}}\n",
    "No prose outside JSON.\n",
    "\"\"\"\n",
    "\n",
    "VALIDATOR_HUMAN: str = \"\"\"Proposed plan JSON:\n",
    "```\n",
    "{plan_json}\n",
    "```\n",
    "\n",
    "Schema preview:\n",
    "```\n",
    "{schema_text}\n",
    "```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "df31c291-4c78-41a9-aee4-802893abe582",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Validation checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cd0185ca-bb3b-4317-9d7d-2049de0baaf8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ValidationResult:\n",
    "    ok: bool\n",
    "    warnings: List[str]\n",
    "    details: Dict[str, Any]\n",
    "\n",
    "def validate_transform(baseline: DataFrame, sample_out: DataFrame, plan: Dict[str,Any]) -> ValidationResult:\n",
    "    warns = []\n",
    "    details = {}\n",
    "    \n",
    "    try:\n",
    "        base_cnt = baseline.count()\n",
    "        out_cnt = sample_out.count()\n",
    "        details[\"row_counts\"] = {\"baseline\": base_cnt, \"output\": out_cnt}\n",
    "\n",
    "        # Safely get expected_effects\n",
    "        expected_effects = plan.get(\"expected_effects\", {})\n",
    "        if not isinstance(expected_effects, dict):\n",
    "            print(f\"WARNING: expected_effects is not a dict: {type(expected_effects)}\")\n",
    "            expected_effects = {}\n",
    "        \n",
    "        # Row count sanity\n",
    "        row_change = expected_effects.get(\"row_change\", \"unknown\")\n",
    "        if isinstance(row_change, str):\n",
    "            row_change = row_change.strip().strip('\"').strip(\"'\").lower()\n",
    "        \n",
    "        if out_cnt == 0 and row_change not in [\"decrease\",\"unknown\"]:\n",
    "            warns.append(\"Output sample row count is 0; this may be unintended.\")\n",
    "        if out_cnt > max(1, base_cnt) * 5:\n",
    "            warns.append(\"Output sample exploded to >5x baseline rows; check joins/explodes.\")\n",
    "\n",
    "        # Column sanity\n",
    "        base_cols = set(baseline.columns)\n",
    "        out_cols = set(sample_out.columns)\n",
    "        new_cols = out_cols - base_cols\n",
    "        dropped = base_cols - out_cols\n",
    "        details[\"columns\"] = {\n",
    "            \"new\": sorted(new_cols),\n",
    "            \"dropped\": sorted(dropped),\n",
    "            \"final\": sorted(out_cols)\n",
    "        }\n",
    "\n",
    "        # Compare with expected - safely handle the list\n",
    "        claimed_new_raw = expected_effects.get(\"new_columns\", [])\n",
    "        if not isinstance(claimed_new_raw, list):\n",
    "            claimed_new_raw = []\n",
    "        # Clean each item in case they have quotes\n",
    "        claimed_new = set()\n",
    "        for item in claimed_new_raw:\n",
    "            if isinstance(item, str):\n",
    "                cleaned = item.strip().strip('\"').strip(\"'\")\n",
    "                claimed_new.add(cleaned)\n",
    "        \n",
    "        if not claimed_new.issuperset(new_cols):\n",
    "            warns.append(f\"Plan under-reported new columns: {sorted(new_cols - claimed_new)}\")\n",
    "\n",
    "        # Null checks for new columns\n",
    "        for c in new_cols:\n",
    "            nulls = sample_out.filter(F.col(c).isNull()).count()\n",
    "            if nulls == out_cnt and out_cnt > 0:\n",
    "                warns.append(f\"New column '{c}' is entirely null in sample.\")\n",
    "\n",
    "        ok = len(warns) == 0\n",
    "        return ValidationResult(ok=ok, warnings=warns, details=details)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in validate_transform: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return ValidationResult(\n",
    "            ok=False,\n",
    "            warnings=[f\"Validation error: {str(e)}\"],\n",
    "            details={\"error\": str(e)}\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dcb84a9b-c443-4ba4-926e-ec1e293959b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### LangGraph state, nodes, and helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "33d41ad6-1aa3-44b7-909d-d1e0f8bb8fe4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    instruction: str\n",
    "    df_in: DataFrame\n",
    "    df_sample: Optional[DataFrame]\n",
    "    plan: Optional[Dict[str, Any]]\n",
    "    code: Optional[str]\n",
    "    validator_report: Optional[Dict[str, Any]]\n",
    "    df_out: Optional[DataFrame]\n",
    "    write_path: str\n",
    "    write_format: str\n",
    "    write_mode: str\n",
    "\n",
    "from typing import Any, Dict\n",
    "from jsonschema import Draft202012Validator\n",
    "\n",
    "def node_plan(state: AgentState, llm: Any) -> Dict[str, Any]:\n",
    "    \"\"\"LLM-driven planning node — generate a safe PySpark transformation plan.\n",
    "\n",
    "    This node is responsible for converting a user's natural-language\n",
    "    instruction into a fully structured execution plan, including:\n",
    "        - a summarized plan description,\n",
    "        - generated PySpark transformation code, and\n",
    "        - expected downstream effects on the DataFrame.\n",
    "\n",
    "    Workflow:\n",
    "        1. Render the planner prompt with schema context.\n",
    "        2. Invoke the LLM to produce a JSON plan.\n",
    "        3. Parse and validate the JSON structure against PLAN_SCHEMA.\n",
    "        4. Sanitize and safety-check the generated PySpark code.\n",
    "\n",
    "    Args:\n",
    "        state: The current AgentState dictionary, containing the input DataFrame\n",
    "               (`df_in`) and the user's instruction.\n",
    "        llm: The planner LLM client (e.g., a Databricks-hosted Chat model).\n",
    "\n",
    "    Returns:\n",
    "        Updated AgentState including:\n",
    "            - `plan`: Validated and cleaned plan JSON.\n",
    "            - `code`: Sanitized PySpark code snippet ready for validation/execution.\n",
    "\n",
    "    Raises:\n",
    "        RuntimeError: If the LLM output cannot be parsed, validated, or fails safety checks.\n",
    "\n",
    "    Example:\n",
    "        >>> new_state = node_plan(state, planner_llm)\n",
    "        >>> print(new_state[\"plan\"][\"summary\"])\n",
    "    \"\"\"\n",
    "    # Extract and format schema context for the prompt\n",
    "    schema_text = schema_preview(state[\"df_in\"])\n",
    "\n",
    "    # Compose planner prompt with schema injection\n",
    "    planner = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", PLANNER_SYS),\n",
    "        (\"human\", PLANNER_HUMAN),\n",
    "    ]).partial(\n",
    "        schema=json.dumps(PLAN_SCHEMA, indent=2),\n",
    "        schema_text=schema_text,\n",
    "    )\n",
    "\n",
    "    # Invoke the planner LLM with the user's natural-language instruction\n",
    "    ai = llm.invoke(planner.format_messages(instruction=state[\"instruction\"]))\n",
    "\n",
    "    try:\n",
    "        # Parse and clean LLM output\n",
    "        plan = parse_llm_json(ai.content)\n",
    "        plan = clean_dict_keys(plan)\n",
    "\n",
    "        # Validate plan structure strictly against PLAN_SCHEMA\n",
    "        Draft202012Validator(PLAN_SCHEMA).validate(plan)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Raw AI response: {ai.content[:1000]}\")\n",
    "        raise RuntimeError(\n",
    "            f\"Planner failed to return valid JSON plan: {e}\\nRaw: {ai.content[:1000]}\"\n",
    "        )\n",
    "\n",
    "    # --- Extract and sanitize PySpark code ---\n",
    "    raw_code = plan.get(\"pyspark_code\", \"\")\n",
    "    if not raw_code:\n",
    "        raise RuntimeError(\"Plan missing 'pyspark_code' field\")\n",
    "\n",
    "    print(f\"DEBUG: Raw code from LLM:\\n{raw_code}\\n\")\n",
    "\n",
    "    code_clean = sanitize_pyspark_code(raw_code)\n",
    "    print(f\"DEBUG: Sanitized code:\\n{code_clean}\\n\")\n",
    "\n",
    "    # --- Safety check ---\n",
    "    ok, why = code_is_safe(code_clean)\n",
    "    if not ok:\n",
    "        print(f\"Failed safety check on code:\\n{code_clean}\")\n",
    "        raise RuntimeError(f\"Proposed code failed safety check: {why}\")\n",
    "\n",
    "    # Return updated state (immutably merged)\n",
    "    return {**state, \"plan\": plan, \"code\": code_clean}\n",
    "\n",
    "def escape_for_template(text: str) -> str:\n",
    "    \"\"\"Escape curly braces for safe inclusion in Python format strings.\n",
    "\n",
    "    Many LLM prompt templates (and f-strings) treat braces `{}` as\n",
    "    placeholders. This helper doubles them (`{{` and `}}`) so literal\n",
    "    braces survive `.format()` substitution without triggering syntax errors.\n",
    "\n",
    "    Args:\n",
    "        text: Raw string that may contain literal `{` or `}` characters.\n",
    "\n",
    "    Returns:\n",
    "        The escaped string where all `{` and `}` have been safely doubled.\n",
    "\n",
    "    Example:\n",
    "        >>> escape_for_template(\"{plan_json}\")\n",
    "        '{{plan_json}}'\n",
    "    \"\"\"\n",
    "    # Double braces so `.format()` interprets them literally.\n",
    "    return text.replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n",
    "\n",
    "from typing import Any, Dict\n",
    "import json\n",
    "import traceback\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "def node_validate(state: AgentState, validator_llm: Any) -> Dict[str, Any]:\n",
    "    \"\"\"Validator node — sanity-check the plan and optionally dry-run on a sample.\n",
    "\n",
    "    Responsibilities\n",
    "    ----------------\n",
    "    1) Build a validator prompt using the plan JSON and a compact schema preview.\n",
    "    2) Invoke the validator LLM and normalize its (often messy) output.\n",
    "    3) If the validator says \"revise\" *and* provides safe `suggested_edits`,\n",
    "       substitute those for the dry-run.\n",
    "    4) If a sample size is configured via a Databricks widget, run a dry-run\n",
    "       validation to catch obvious issues (row explosions, null-only new columns, etc.).\n",
    "\n",
    "    Behavior\n",
    "    --------\n",
    "    - Returns an updated state with `validator_report` and possibly updated `code`.\n",
    "    - On any exception, returns a conservative 'revise' verdict with diagnostic info.\n",
    "    - Does not persist or mutate external systems; purely in-memory checks.\n",
    "\n",
    "    Args:\n",
    "        state: Current AgentState dict (expects `df_in`, `plan`, and `code` at minimum).\n",
    "        validator_llm: LLM client used to validate safety/compatibility/effects.\n",
    "\n",
    "    Returns:\n",
    "        A new AgentState dict with:\n",
    "          - `validator_report`: normalized validator output (and optional dry-run results)\n",
    "          - `code`: possibly updated code if safe suggested edits were provided\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"ENTERING node_validate\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    try:\n",
    "        # --- Step 1: Schema preview for validator context ---\n",
    "        print(\"Step 1: Getting schema preview...\")\n",
    "        schema_text = schema_preview(state[\"df_in\"])\n",
    "        print(f\"Schema preview length: {len(schema_text)}\")\n",
    "        \n",
    "        # --- Step 2: Build validator prompt ---\n",
    "        print(\"Step 2: Building prompt...\")\n",
    "        vprompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", VALIDATOR_SYS),\n",
    "            (\"human\", VALIDATOR_HUMAN),\n",
    "        ])\n",
    "        \n",
    "        # --- Step 3: Format messages (escape braces in plan JSON) ---\n",
    "        print(\"Step 3: Formatting messages...\")\n",
    "        plan_json_str = json.dumps(state[\"plan\"], indent=2)\n",
    "        plan_json_escaped = escape_for_template(plan_json_str)\n",
    "        messages = vprompt.format_messages(\n",
    "            plan_json=plan_json_escaped,\n",
    "            schema_text=schema_text,\n",
    "        )\n",
    "        print(f\"Formatted {len(messages)} messages\")\n",
    "        \n",
    "        # --- Step 4: Invoke validator LLM ---\n",
    "        print(\"Step 4: Invoking validator LLM...\")\n",
    "        ai = validator_llm.invoke(messages)\n",
    "        print(f\"LLM response received, length: {len(ai.content)}\")\n",
    "        print(f\"DEBUG: Raw validator response: {ai.content[:500]}\")\n",
    "        \n",
    "        # --- Step 5: Normalize/clean validator report ---\n",
    "        print(\"Step 5: Normalizing validator report...\")\n",
    "        report = normalize_validator_report(ai.content)\n",
    "        print(\"Step 6: Normalization complete\")\n",
    "        \n",
    "        print(\"Step 7: Cleaning dict keys...\")\n",
    "        report = clean_dict_keys(report)\n",
    "        print(\"Step 8: Cleaning complete\")\n",
    "        \n",
    "        print(f\"DEBUG: Normalized report keys: {list(report.keys())}\")\n",
    "        print(f\"DEBUG: Verdict value: '{report.get('verdict')}' (type: {type(report.get('verdict'))})\")\n",
    "        \n",
    "        # Ensure required keys exist\n",
    "        if \"verdict\" not in report:\n",
    "            print(\"WARNING: verdict missing from report, defaulting to 'revise'\")\n",
    "            report[\"verdict\"] = \"revise\"\n",
    "        if \"reasons\" not in report:\n",
    "            report[\"reasons\"] = \"Validator response incomplete\"\n",
    "        if \"suggested_edits\" not in report:\n",
    "            report[\"suggested_edits\"] = None\n",
    "        \n",
    "        # Prefer safe suggested edits if validator requested a revision\n",
    "        code_to_try = state[\"code\"]\n",
    "        if report[\"verdict\"] == \"revise\" and report.get(\"suggested_edits\"):\n",
    "            candidate = sanitize_pyspark_code(report[\"suggested_edits\"])\n",
    "            ok, why = code_is_safe(candidate)\n",
    "            if ok:\n",
    "                code_to_try = candidate\n",
    "        \n",
    "        # --- Optional dry-run sample validation (Databricks widget controlled) ---\n",
    "        # Keep the widget name exactly as used operationally.\n",
    "        try:\n",
    "            n = int(dbutils.widgets.get(\"Sample Rows\") or \"0\")  # noqa: F821\n",
    "        except Exception:\n",
    "            n = 0\n",
    "        \n",
    "        df_sample = state[\"df_in\"].limit(n) if n > 0 else None\n",
    "        if df_sample is not None and df_sample.count() == 0:\n",
    "            df_sample = None\n",
    "        \n",
    "        if df_sample is not None:\n",
    "            try:\n",
    "                sample_out = exec_pyspark_code(code_to_try, df_sample)\n",
    "                vr = validate_transform(df_sample, sample_out, state[\"plan\"])\n",
    "                report[\"dry_run\"] = {\n",
    "                    \"ok\": vr.ok,\n",
    "                    \"warnings\": vr.warnings,\n",
    "                    \"details\": vr.details,\n",
    "                }\n",
    "                if not vr.ok:\n",
    "                    report[\"verdict\"] = \"revise\"\n",
    "                    report[\"reasons\"] = \" ; \".join(vr.warnings) if vr.warnings else \"Validation failed\"\n",
    "            except Exception as e:\n",
    "                report[\"verdict\"] = \"revise\"\n",
    "                report[\"reasons\"] = f\"Code failed on sample: {e}\"\n",
    "                report[\"dry_run\"] = {\"ok\": False, \"error\": str(e)}\n",
    "        \n",
    "        return {**state, \"validator_report\": report, \"code\": code_to_try}\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"CRITICAL ERROR in node_validate: {e}\")\n",
    "        traceback.print_exc()\n",
    "        # Conservative fallback: return a structured \"revise\" verdict\n",
    "        return {\n",
    "            **state,\n",
    "            \"validator_report\": {\n",
    "                \"verdict\": \"revise\",\n",
    "                \"reasons\": f\"Validator node crashed: {str(e)}\",\n",
    "                \"suggested_edits\": None,\n",
    "            },\n",
    "        }\n",
    "\n",
    "def node_execute(state: AgentState) -> Dict[str, Any]:\n",
    "    \"\"\"Execution node — run the validated PySpark code over the full dataset.\n",
    "\n",
    "    Assumptions\n",
    "    -----------\n",
    "    - `state[\"code\"]` is sanitized and has already passed safety checks.\n",
    "    - `state[\"df_in\"]` is a valid Spark DataFrame.\n",
    "    - The executed snippet must assign its result to `df_out`.\n",
    "\n",
    "    Behavior\n",
    "    --------\n",
    "    Executes the code in a constrained namespace (`df`, `F`) and returns a new\n",
    "    state with `df_out` attached. If execution fails, we raise a `RuntimeError`\n",
    "    with the original exception and the attempted code for fast debugging.\n",
    "\n",
    "    Args:\n",
    "        state: Current AgentState dict containing `code` and `df_in`.\n",
    "\n",
    "    Returns:\n",
    "        A new AgentState dict with `df_out` set to the execution result.\n",
    "\n",
    "    Raises:\n",
    "        RuntimeError: If the snippet fails to execute or does not yield `df_out`.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df_out = exec_pyspark_code(state[\"code\"], state[\"df_in\"])\n",
    "        return {**state, \"df_out\": df_out}\n",
    "    except Exception as e:\n",
    "        # Fail loudly with enough context to debug quickly.\n",
    "        raise RuntimeError(f\"Execution failed: {e}\\nCode:\\n{state['code']}\") from e\n",
    "\n",
    "def node_write(state: AgentState) -> Dict[str, Any]:\n",
    "    \"\"\"Write the output DataFrame to persistent storage.\n",
    "\n",
    "    This node handles the final persistence step in the LangGraph workflow.\n",
    "    It takes the `df_out` DataFrame produced by the execution node and writes it\n",
    "    to the configured output path using the specified format and mode.\n",
    "\n",
    "    Design intent\n",
    "    --------------\n",
    "    - Enforce explicit control over write paths, formats, and modes.\n",
    "    - Guard against missing outputs (we never silently skip writes).\n",
    "    - Normalize DBFS paths so the same logic works for `/dbfs/` and `dbfs:/` URIs.\n",
    "    - Keep format routing simple — just Spark-native writers (no spark.sql).\n",
    "\n",
    "    Args:\n",
    "        state: Current AgentState dict containing the keys:\n",
    "            - `df_out`: DataFrame to write.\n",
    "            - `write_path`: Destination path (DBFS/S3/ABFSS/etc.).\n",
    "            - `write_format`: Output format (\"delta\", \"parquet\", \"csv\", \"json\").\n",
    "            - `write_mode`: Write mode (\"overwrite\", \"append\", etc.).\n",
    "\n",
    "    Returns:\n",
    "        The unmodified AgentState dict after successful write.\n",
    "\n",
    "    Raises:\n",
    "        RuntimeError: If `df_out` is missing or write operation fails.\n",
    "\n",
    "    Example:\n",
    "        >>> node_write({\n",
    "        ...     \"df_out\": df,\n",
    "        ...     \"write_path\": \"/dbfs/tmp/output\",\n",
    "        ...     \"write_format\": \"delta\",\n",
    "        ...     \"write_mode\": \"overwrite\"\n",
    "        ... })\n",
    "        Successfully wrote output to dbfs:/tmp/output in delta format\n",
    "    \"\"\"\n",
    "    # --- Sanity check ---\n",
    "    if not state.get(\"df_out\"):\n",
    "        raise RuntimeError(\"No output DataFrame to write\")\n",
    "\n",
    "    # Normalize DBFS path so Spark understands both /dbfs/ and dbfs:/ conventions\n",
    "    path = _normalize_dbfs_path(state[\"write_path\"])\n",
    "    fmt = state.get(\"write_format\", \"parquet\")\n",
    "    mode = state.get(\"write_mode\", \"overwrite\")\n",
    "\n",
    "    writer = state[\"df_out\"].write.mode(mode)\n",
    "\n",
    "    # --- Format routing ---\n",
    "    # Keep this explicit; we don’t use dynamic evals for safety.\n",
    "    if fmt == \"delta\":\n",
    "        writer.format(\"delta\").save(path)\n",
    "    elif fmt == \"parquet\":\n",
    "        writer.parquet(path)\n",
    "    elif fmt == \"json\":\n",
    "        writer.json(path)\n",
    "    elif fmt == \"csv\":\n",
    "        writer.option(\"header\", True).csv(path)\n",
    "    else:\n",
    "        # Default fallback for unknown formats\n",
    "        writer.parquet(path)\n",
    "\n",
    "    print(f\"Successfully wrote output to {path} in {fmt} format\")\n",
    "    return state\n",
    "\n",
    "from typing import Any\n",
    "import traceback\n",
    "\n",
    "def safe_get_verdict(state: AgentState) -> str:\n",
    "    \"\"\"Extract a normalized verdict from the validator report for graph routing.\n",
    "\n",
    "    This function is used by LangGraph’s conditional edge logic to determine\n",
    "    whether the workflow should proceed to execution or stop for revision.\n",
    "\n",
    "    Behavior\n",
    "    --------\n",
    "    - Returns `\"approve\"` only if the validator explicitly says so.\n",
    "    - Defaults to `\"revise\"` in all other cases (including missing or malformed state).\n",
    "    - Strips accidental quotes or whitespace that often appear in raw LLM outputs.\n",
    "\n",
    "    Args:\n",
    "        state: Current AgentState dictionary, expected to contain\n",
    "               a `validator_report` with a `\"verdict\"` key.\n",
    "\n",
    "    Returns:\n",
    "        `\"approve\"` if the validator report explicitly approves the plan;\n",
    "        otherwise `\"revise\"` (safe fallback).\n",
    "\n",
    "    Example:\n",
    "        >>> safe_get_verdict({\"validator_report\": {\"verdict\": \"approve\"}})\n",
    "        'approve'\n",
    "        >>> safe_get_verdict({})\n",
    "        'revise'\n",
    "    \"\"\"\n",
    "    try:\n",
    "        report = state.get(\"validator_report\")\n",
    "        if not report:\n",
    "            print(\"No validator_report in state, defaulting to 'revise'\")\n",
    "            return \"revise\"\n",
    "\n",
    "        verdict = report.get(\"verdict\", \"revise\")\n",
    "\n",
    "        # Normalize verdict string for robustness\n",
    "        if isinstance(verdict, str):\n",
    "            verdict = verdict.strip().strip('\"').strip(\"'\").lower()\n",
    "\n",
    "        result = \"approve\" if verdict == \"approve\" else \"revise\"\n",
    "        print(f\"Routing decision: {result}\")\n",
    "        return result\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting verdict: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return \"revise\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "80cd813f-4aaf-49a5-95fc-a34b770eb6ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Graph builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4f356b46-6074-46f2-beae-5f87f08bd0df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def build_graph(planner_llm: ChatDatabricks, validator_llm: ChatDatabricks) -> Any:\n",
    "    \"\"\"Assemble and compile the LangGraph workflow (plan → validate → execute → write).\n",
    "    \n",
    "    Args:\n",
    "        planner_llm: Chat model used to generate the PySpark plan/code.\n",
    "        validator_llm: Chat model used to validate safety/compatibility/effects.\n",
    "\n",
    "    Returns:\n",
    "        A compiled LangGraph workflow object ready to `.invoke(initial_state)`.\n",
    "\n",
    "    Example:\n",
    "        >>> graph = build_graph(planner_llm, validator_llm)\n",
    "        >>> result = graph.invoke(initial_state)\n",
    "    \"\"\"\n",
    "    # Create the stateful workflow over the AgentState TypedDict contract.\n",
    "    workflow = StateGraph(AgentState)\n",
    "\n",
    "    # Nodes\n",
    "    # Capture LLMs in closures so node functions stay pure wrt global state.\n",
    "    workflow.add_node(\"plan\",     lambda s: node_plan(s, planner_llm))\n",
    "    workflow.add_node(\"validate\", lambda s: node_validate(s, validator_llm))\n",
    "    workflow.add_node(\"execute\",  node_execute)\n",
    "    workflow.add_node(\"write\",    node_write)\n",
    "\n",
    "    # Linear edges: START → plan → validate\n",
    "    workflow.add_edge(START, \"plan\")\n",
    "    workflow.add_edge(\"plan\", \"validate\")\n",
    "\n",
    "    # Conditional routing after validation:\n",
    "    #   - approve → execute\n",
    "    #   - revise  → END (could loop back to \"plan\" if you add retry semantics later)\n",
    "    workflow.add_conditional_edges(\n",
    "        \"validate\",\n",
    "        safe_get_verdict,\n",
    "        {\n",
    "            \"approve\": \"execute\",\n",
    "            \"revise\": END,  # intentional early stop on non-approved code\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # Final leg: execute → write → END\n",
    "    workflow.add_edge(\"execute\", \"write\")\n",
    "    workflow.add_edge(\"write\", END)\n",
    "\n",
    "    # Compile to an executable graph.\n",
    "    return workflow.compile()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e3c73558-429e-424f-a45a-664ac5d245e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Main execution function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4c259934-86d7-4697-811d-637a959051d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def run_transformation(\n",
    "    instruction: str,\n",
    "    input_paths: str,\n",
    "    output_path: str,\n",
    "    input_format: str = \"auto\",\n",
    "    output_format: str = \"delta\",\n",
    "    write_mode: str = \"overwrite\",\n",
    "    planner_provider: str = \"openai\",\n",
    "    validator_provider: str = \"openai\",\n",
    "    temperature: float = 0.0\n",
    ") -> Optional[DataFrame]:\n",
    "    \"\"\"Plan, validate, execute, and persist a PySpark transformation.\n",
    "\n",
    "    This is the single entry point that wires the whole workflow together:\n",
    "    - Load inputs (tolerant to schema drift).\n",
    "    - Spin up planner/validator LLM handles.\n",
    "    - Build the LangGraph (plan → validate → execute → write).\n",
    "    - Invoke the graph with a minimal initial state.\n",
    "    - Print a concise run summary for quick inspection.\n",
    "\n",
    "    Args:\n",
    "        instruction: Natural-language description of the desired transformation.\n",
    "        input_paths: Comma-separated list of input paths (DBFS/S3/ABFSS/etc.).\n",
    "        output_path: Destination path for the final dataset.\n",
    "        input_format: Input format or \"auto\" (parquet|delta|csv|json).\n",
    "        output_format: Output format (parquet|delta|csv|json).\n",
    "        write_mode: Spark write mode (e.g., \"overwrite\", \"append\").\n",
    "        planner_provider: LLM provider for planning (\"openai\"|\"anthropic\"|\"llama\").\n",
    "        validator_provider: LLM provider for validation.\n",
    "        temperature: LLM decoding temperature.\n",
    "\n",
    "    Returns:\n",
    "        The final `DataFrame` (if execution reached the execute/write stages), else `None`.\n",
    "\n",
    "    Notes:\n",
    "        - Requires an active `spark` session in the runtime (Databricks notebooks inject it).\n",
    "        - This function prints human-readable checkpoints by design for operational clarity.\n",
    "    \"\"\"\n",
    "    # Load input data (handles multi-path union by name with missing columns filled)\n",
    "    print(f\"Loading data from: {input_paths}\")\n",
    "    df_in: DataFrame = load_inputs(input_paths, input_format)\n",
    "    print(f\"Loaded DataFrame with {df_in.count()} rows and {len(df_in.columns)} columns\")\n",
    "\n",
    "    # Create LLM clients (endpoints are configured in make_llm)\n",
    "    planner_llm = make_llm(planner_provider, temperature)\n",
    "    validator_llm = make_llm(validator_provider, temperature)\n",
    "\n",
    "    # Build the orchestration graph\n",
    "    graph = build_graph(planner_llm, validator_llm)\n",
    "\n",
    "    # Initial state passed into the graph (intentionally minimal and explicit)\n",
    "    initial_state: Dict[str, Any] = {\n",
    "        \"instruction\": instruction,\n",
    "        \"df_in\": df_in,\n",
    "        \"df_sample\": None,\n",
    "        \"plan\": None,\n",
    "        \"code\": None,\n",
    "        \"validator_report\": None,\n",
    "        \"df_out\": None,\n",
    "        \"write_path\": output_path,\n",
    "        \"write_format\": output_format,\n",
    "        \"write_mode\": write_mode,\n",
    "    }\n",
    "\n",
    "    # Execute the graph\n",
    "    print(\"\\n=== Starting transformation ===\")\n",
    "    result: Dict[str, Any] = graph.invoke(initial_state)\n",
    "\n",
    "    # Summarize plan and validation for quick operator feedback\n",
    "    print(\"\\n=== Plan Summary ===\")\n",
    "    if result.get(\"plan\"):\n",
    "        print(result[\"plan\"].get(\"summary\", \"No summary available\"))\n",
    "\n",
    "    print(\"\\n=== Validator Report ===\")\n",
    "    if result.get(\"validator_report\"):\n",
    "        report = result[\"validator_report\"]\n",
    "        print(f\"Verdict: {report.get('verdict', 'unknown')}\")\n",
    "        print(f\"Reasons: {report.get('reasons', 'none')}\")\n",
    "        if report.get(\"dry_run\"):\n",
    "            print(f\"Dry run: {report['dry_run']}\")\n",
    "\n",
    "    print(\"\\n=== Execution Complete ===\")\n",
    "    if result.get(\"df_out\"):\n",
    "        df_out: DataFrame = result[\"df_out\"]\n",
    "        print(f\"Output: {df_out.count()} rows, {len(df_out.columns)} columns\")\n",
    "        print(f\"Schema: {df_out.printSchema()}\")\n",
    "\n",
    "    return result[\"df_out\"] if result.get(\"df_out\") else None\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "pyspark-agent-dependencies",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
